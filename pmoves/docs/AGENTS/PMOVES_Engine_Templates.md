# **PMOVES.AI TTS Templates**

## **1\. KOKORO TTS TEMPLATE (The "Host")**

Best For: Natural flow, long introductions, warm narration.  
Technique: Punctuation Engineering.

* ... \= Long Pause (approx 600ms)  
* — \= Sharp break/Tone shift  
* , \= Short breath

\[Copy-Paste This Text Block\]  
Imagine a world where your local hardware isn't just a server... but a living organism.  
We are looking at the P-MOVES dot A-I orchestration mesh. It’s a distributed architecture—one that mirrors high-end production environments—but it lives entirely on your local metal.

No cloud... No leaks... Just raw, autonomous power.

So... how does it actually think? You mentioned a distinction between "Brains" and "Muscles"?

This is the future of the "fully automated industrial substrate." Agents speaking to agents. Protocols replacing meetings. And all of it... running privately, under your control.

Welcome... to P-MOVES dot A-I.

## **2\. FISH SPEECH TEMPLATE (The "Architect")**

Best For: High-speed technical jargon, excitement, manic energy.  
Technique: Reference Audio Cloning.

### **Block A: The Infrastructure**

* **Reference Audio Strategy:** Upload a 10-second clip of a fast-talking tech reviewer (e.g., Linus Tech Tips or a rapid tutorial).  
* **Reference Text (If required by UI):** "This is the fastest CPU we have ever tested and it is absolutely mind blowing."

\[Target Text to Copy\]  
Correct. And this is not just storage... this is Local Inference\!  
We utilize Tensor-Zero to host Qwen models—ranging from four billion to thirty-two billion parameters—right on the edge. This ensures that the agent's intelligence remains on-site.

### **Block B: The Logic**

* **Reference Audio Strategy:** Same clip, or something even faster.

\[Target Text to Copy\]  
Precisely\! The mesh is bifurcated.  
First, the Decision Engine—Agent Zero. It acts as an M-C-P bridge. It ingests events, processes context, and triggers the muscles.  
Then... you have Archon, the Knowledge Manager. It coordinates with NATS and Supabase Realtime to maintain state stability across the entire mesh.

### **Block C: The Mesh**

\[Target Text to Copy\]  
We call it the Unified Communication Mesh.  
Agents coordinate via NATS messaging. We can actually observe the system "thinking" through events like claude.code.tool.executed.v1.  
But the real breakthrough? Contextual Geometry Packets—or CGPs.

## **3\. INDEXTTS2 TEMPLATE (The "Ops/Engineer")**

Best For: Texture, grit, specific emotional control (whispers, authority).  
Technique: Natural Language Style Prompts.

### **Block A: The Hardware**

* **Style/Emotion Prompt:** Low pitch, slow speed, gravelly voice, serious

\[Target Text to Copy\]  
It starts with the Local-First Infrastructure. We aren't renting power. We are provisioning it.  
Inside the Cataclysm Studios Inc directory, you find the blueprint. We have automated OS installs, Jetson bootstrap scripts, and Docker stacks.  
This allows edge hardware to mirror the production topology... exactly.

### **Block B: The Muscles**

* **Style/Emotion Prompt:** Normal pitch, authoritative, slightly faster

\[Target Text to Copy\]  
And then... the muscles do the heavy lifting.  
Services like P-Moves-Y-T for media ingestion, Deep-Research for data loops, and N-eight-N for workflow orchestration. They are autonomous services, feeding data back into the core.

### **Block C: The Self-Improvement**

* **Style/Emotion Prompt:** Deep voice, whisper, intense

\[Target Text to Copy\]  
And it learns.  
The Evo-Controller reads those packets and emits "tuning capsules" back into the bus.  
It creates a self-improving loop. We even run a full smoke-test harness—make verify-all—to validate every agent locally before we commit a single line of code.

## **4\. VIBEVOICE TEMPLATE (The "Podcast" Mode)**

Best For: Generating the entire conversation in one go (if your UI supports Multi-Speaker Inference).  
Technique: Speaker Labeling.  
**\[Copy-Paste into "Multi-Speaker" or "Chat" Tab\]**

**Speaker 1 (Host):** Imagine a world where your local hardware isn't just a server, but a living organism. We are looking at the P-MOVES dot A-I orchestration mesh.

**Speaker 2 (Ops):** It starts with the Local-First Infrastructure. We aren't renting power; we are provisioning it. Inside the directory, you find the blueprint. Automated OS installs. Jetson bootstrap scripts.

**Speaker 3 (Architect):** Correct\! And this is not just storage. This is Local Inference. We utilize Tensor-Zero to host Qwen models right on the edge.

**Speaker 2 (Ops):** Plus, with "Single-User Mode" and boot-J-W-T auto-authentication, the system operates offline. No external login prompts.

**Speaker 1 (Host):** So, how does it actually think?

**Speaker 3 (Architect):** Precisely. The mesh is bifurcated. First, the Decision Engine—Agent Zero. It acts as an M-C-P bridge. Then, you have Archon, the Knowledge Manager.

**Speaker 2 (Ops):** And then the muscles do the heavy lifting. Services like Deep-Research and N-eight-N.

**Speaker 3 (Architect):** We call it the Unified Communication Mesh. Agents coordinate via NATS messaging. But the real breakthrough? Contextual Geometry Packets.

**Speaker 1 (Host):** This is the future of the fully automated industrial substrate. Welcome to PMOVES.AI.